id,text,option1,option2,option3,option4,correctAnswer,researchPaperId
1,"What is the primary subject of the paper 'Attention Is All You Need'?",Transformers,Convolutional Neural Networks,Recurrent Neural Networks,Autoencoders,Transformers,RP001
2,"According to the paper, what mechanism allows for modeling dependencies without regard to their distance?",Self-attention,Positional encoding,Multi-head attention,Scaled dot-product attention,Self-attention,RP001
3,"Which model architecture is the Transformer primarily proposed as an alternative to?",RNNs and LSTMs,CNNs,Feed-forward networks,Support Vector Machines,RNNs and LSTMs,RP001
4,"What are 'positional encodings' used for in the Transformer model?","To give the model some information about the relative or absolute position of the tokens in the sequence.",To increase the model's vocabulary size.,To reduce the dimensionality of the input embeddings.,To provide a secondary attention mechanism.,To give the model some information about the relative or absolute position of the tokens in the sequence.,RP001
5,"What is the role of 'multi-head attention'?",,"It allows the model to jointly attend to information from different representation subspaces at different positions.",It ensures the model only attends to a single position at a time.,It combines the outputs of multiple Transformer blocks.,It is a form of regularization to prevent overfitting.,It allows the model to jointly attend to information from different representation subspaces at different positions.,RP001
6,"What is the title of the famous 2017 paper that introduced the Transformer architecture?","Attention Is All You Need","BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","GPT-3: Language Models are Few-Shot Learners","Efficiently Building a Machine Translation System", "Attention Is All You Need", RP002
7,"What does the 'self-attention' mechanism in Transformers compute?","The dot product between the query and all keys","A weighted sum of the values, where the weight is determined by the dot product of the query with all keys","The softmax of the concatenated query and key vectors","A simple average of all value vectors","A weighted sum of the values, where the weight is determined by the dot product of the query with all keys",RP002
8,"In the Transformer architecture, what is the purpose of the feed-forward network applied after the attention layers?","To introduce non-linearity and process the attention outputs","To reduce the dimensionality of the model","To generate the final output probabilities","To normalize the outputs of the attention layer","To introduce non-linearity and process the attention outputs",RP002
9,"How does the Transformer handle input sequences of varying lengths?","It uses a fixed-size context window","It truncates or pads all sequences to a fixed length","It processes sequences token-by-token with no inherent length limitation (up to memory constraints)","It requires all input sequences to be of the same length","It processes sequences token-by-token with no inherent length limitation (up to memory constraints)",RP002
10,"What is a key advantage of the Transformer architecture over RNNs in terms of computation?","It allows for significantly more parallelization","It requires less memory","It is guaranteed to converge faster","It has fewer parameters","It allows for significantly more parallelization",RP002
